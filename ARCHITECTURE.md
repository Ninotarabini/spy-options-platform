# √∞≈∏‚Äî√Ø¬∏ SPY Options Platform - Architecture Deep Dive

## Table of Contents

- [Design Philosophy](#design-philosophy)
- [System Architecture](#system-architecture)
- [Component Details](#component-details)
- [Network Topology](#network-topology)
- [Data Flow](#data-flow)
- [Technical Decisions](#technical-decisions)
- [Scalability & Resilience](#scalability--resilience)
- [Security Architecture](#security-architecture)
- [Monitoring & Observability](#monitoring--observability)

---

## √∞≈∏¬ß  Design Philosophy

### Core Principles

1. **Hybrid by Design**: Leverage cloud for management and observability, edge for performance-critical execution
2. **Infrastructure as Code**: 100% reproducible, version-controlled infrastructure
3. **Container-First**: All applications containerized for portability and consistency
4. **Declarative Configuration**: Kubernetes and Terraform manage desired state
5. **Observable by Default**: Comprehensive metrics, logs, and traces from day one

### Architecture Goals

| Goal | Implementation | Metric |
|------|---------------|--------|
| **Low Latency** | Edge execution, local IBKR connection | <500ms end-to-end |
| **High Availability** | 3-replica deployment, auto-healing | 99.9% uptime |
| **Cost Efficiency** | Hybrid architecture, optimized tiers | $62.50/month |
| **Maintainability** | IaC, GitOps, declarative configs | Single-command deploy |
| **Security** | Key Vault, VPN, RBAC, scanning | Zero-trust model |

---

## üèóÔ∏è System Architecture

### Three-Layer Architecture
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    CONTROL PLANE (Azure)                      ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ  ‚îÇ   Terraform ‚îÇ  ‚îÇ  Container  ‚îÇ  ‚îÇ     App     ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ   Modules   ‚îÇ  ‚îÇ   Registry  ‚îÇ  ‚îÇ   Service   ‚îÇ         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ  ‚îÇ   SignalR   ‚îÇ  ‚îÇ     Key     ‚îÇ  ‚îÇ Application ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ   Service   ‚îÇ  ‚îÇ    Vault    ‚îÇ  ‚îÇ  Insights   ‚îÇ         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚îÇ VPN Gateway (IPsec)
                         ‚îÇ 10.0.0.0/16 ‚Üî 192.168.1.0/24
                         ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    DATA PLANE (On-Premises)                   ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ         Kubernetes Cluster (k3s / minikube)            ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                                         ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  Namespace: trading-bots                               ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  Trading Bot  ‚îÇ  ‚îÇ  Trading Bot  ‚îÇ  (3 replicas)   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ   (Pod 1/3)   ‚îÇ  ‚îÇ   (Pod 2/3)   ‚îÇ                 ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                                         ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ    IBKR Gateway (StatefulSet)    ‚îÇ                 ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ    Persistent Connection         ‚îÇ                 ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                                         ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  Namespace: monitoring                                 ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  Prometheus  ‚îÇ  ‚îÇ   Grafana    ‚îÇ                  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üîß Component Details

### 1. Azure Infrastructure (Terraform-Managed)

#### Virtual Network Architecture
```
VNet: 10.0.0.0/16 (65,536 addresses)
‚îú‚îÄ‚îÄ GatewaySubnet: 10.0.0.0/27 (32 addresses)
‚îÇ   ‚îî‚îÄ‚îÄ VPN Gateway (Basic SKU)
‚îÇ       ‚Ä¢ IKEv2 protocol
‚îÇ       ‚Ä¢ 100 Mbps throughput
‚îÇ       ‚Ä¢ Pre-shared key authentication
‚îÇ
‚îú‚îÄ‚îÄ AppSubnet: 10.0.1.0/24 (256 addresses)
‚îÇ   ‚îú‚îÄ‚îÄ App Service (B1)
‚îÇ   ‚îÇ   ‚Ä¢ 1 vCPU, 1.75 GB RAM
‚îÇ   ‚îÇ   ‚Ä¢ Python 3.11 runtime
‚îÇ   ‚îÇ   ‚Ä¢ Always On enabled
‚îÇ   ‚îî‚îÄ‚îÄ VNet Integration
‚îÇ
‚îî‚îÄ‚îÄ ContainerSubnet: 10.0.2.0/24 (256 addresses)
    ‚îî‚îÄ‚îÄ Container Instances (Optional)
```

#### Network Security Groups
```terraform
# Inbound Rules (Priority ascending = higher precedence)
Priority 100: Allow VPN (UDP 500, 4500) from on-premises IP
Priority 110: Allow HTTPS (443) from Internet to App Service
Priority 120: Deny All other inbound

# Outbound Rules
Priority 100: Allow All to VNet
Priority 110: Allow All to Internet (for package downloads)
```

#### Container Registry Configuration

- **SKU**: Basic ($5/month)
- **Storage**: 10 GB included
- **Features**: Admin enabled, webhooks (2), geo-replication (disabled)
- **Images**:
  - `spy-backend:latest` / `spy-backend:v1.x`
  - `spy-trading-bot:latest` / `spy-trading-bot:v1.x`
  - `spy-detector:latest` / `spy-detector:v1.x`

#### App Service Configuration
```yaml
App Service Plan:
  Tier: Basic (B1)
  Compute: 1 vCPU, 1.75 GB RAM
  OS: Linux
  Storage: 10 GB

Web App:
  Runtime: Python 3.11
  Startup Command: uvicorn app:app --host 0.0.0.0 --port 8000
  Always On: true
  Health Check: /health (30s interval)
  Environment Variables:
    - IBKR_HOST: ibkr-gateway.trading-bots.svc.cluster.local
    - SIGNALR_CONNECTION_STRING: [from Key Vault]
    - LOG_LEVEL: INFO
```

### 2. Kubernetes Cluster (On-Premises)

#### Cluster Specifications

| Component | Specification | Notes |
|-----------|--------------|-------|
| **Distribution** | k3s (Linux) / minikube (Windows) | Single-node cluster |
| **Version** | 1.28+ | Latest stable |
| **Runtime** | Docker 24+ | containerd alternative supported |
| **CNI** | Flannel (k3s) / Bridge (minikube) | Default networking |
| **LoadBalancer** | MetalLB | Bare-metal LB implementation |
| **Storage** | Local path provisioner | HostPath-based PVs |

#### Namespace Architecture
```yaml
Namespaces:
  trading-bots:
    Purpose: Production trading workloads
    Resources:
      - Deployment: trading-bot (3 replicas)
      - StatefulSet: ibkr-gateway (1 replica)
      - Service: trading-bot-service (ClusterIP)
      - Service: ibkr-gateway-service (ClusterIP)
      - ConfigMap: bot-config, trading-strategies
      - Secret: ibkr-credentials, acr-registry
      - PVC: ibkr-data (5GB), sqlite-db (10GB)

  monitoring:
    Purpose: Observability stack
    Resources:
      - Deployment: prometheus-server
      - Deployment: grafana
      - DaemonSet: fluentd
      - Service: prometheus (LoadBalancer)
      - Service: grafana (LoadBalancer)
      - ConfigMap: prometheus-config, grafana-dashboards

  system:
    Purpose: Cluster infrastructure
    Resources:
      - DaemonSet: node-exporter
      - Deployment: metrics-server
```

#### Trading Bot Deployment
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: trading-bot
  namespace: trading-bots
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0  # Zero-downtime updates
  
  template:
    spec:
      containers:
      - name: bot
        image: acr.azurecr.io/spy-trading-bot:v1.2
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 2
        
        env:
        - name: IBKR_HOST
          value: "ibkr-gateway-0.ibkr-gateway-service"
        - name: IBKR_PORT
          value: "4002"
        - name: SIGNALR_URL
          valueFrom:
            secretKeyRef:
              name: azure-secrets
              key: signalr-url
```

#### IBKR Gateway StatefulSet
```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: ibkr-gateway
  namespace: trading-bots
spec:
  serviceName: ibkr-gateway-service
  replicas: 1
  
  volumeClaimTemplates:
  - metadata:
      name: ibkr-data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 5Gi
  
  template:
    spec:
      containers:
      - name: gateway
        image: ghcr.io/gnzsnz/ib-gateway:stable
        ports:
        - containerPort: 4002
          name: api
        volumeMounts:
        - name: ibkr-data
          mountPath: /root/Jts
        - name: config
          mountPath: /root/conf.yaml
          subPath: conf.yaml
      
      volumes:
      - name: config
        configMap:
          name: ibkr-config
```

### 3. Helm Chart Structure
```
helm/spy-trading-bot/
‚îú‚îÄ‚îÄ Chart.yaml              # Metadata (name, version, appVersion)
‚îú‚îÄ‚îÄ values.yaml             # Default values
‚îú‚îÄ‚îÄ values-dev.yaml         # Development overrides
‚îú‚îÄ‚îÄ values-prod.yaml        # Production overrides
‚îÇ
‚îî‚îÄ‚îÄ templates/
    ‚îú‚îÄ‚îÄ deployment.yaml     # Trading bot deployment
    ‚îú‚îÄ‚îÄ statefulset.yaml    # IBKR gateway
    ‚îú‚îÄ‚îÄ service.yaml        # ClusterIP services
    ‚îú‚îÄ‚îÄ configmap.yaml      # Configuration data
    ‚îú‚îÄ‚îÄ secret.yaml         # Sensitive data (templated)
    ‚îú‚îÄ‚îÄ pvc.yaml            # Persistent volume claims
    ‚îú‚îÄ‚îÄ servicemonitor.yaml # Prometheus scraping
    ‚îú‚îÄ‚îÄ _helpers.tpl        # Template functions
    ‚îî‚îÄ‚îÄ NOTES.txt           # Post-install instructions
```

#### Values Hierarchy
```yaml
# values.yaml (defaults)
replicaCount: 2
image:
  repository: acr.azurecr.io/spy-trading-bot
  tag: "latest"
resources:
  requests:
    memory: "256Mi"
    cpu: "250m"

# values-prod.yaml (overrides)
replicaCount: 3
image:
  tag: "v1.2"
resources:
  limits:
    memory: "512Mi"
    cpu: "500m"
```

---

## üåê Network Topology

### VPN Site-to-Site Configuration
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Azure VNet     ‚îÇ                    ‚îÇ  On-Premises    ‚îÇ
‚îÇ  10.0.0.0/16    ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄIPsec‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ  192.168.1.0/24 ‚îÇ
‚îÇ                 ‚îÇ                    ‚îÇ                 ‚îÇ
‚îÇ  VPN Gateway    ‚îÇ                    ‚îÇ  Edge Gateway   ‚îÇ
‚îÇ  Public IP: X   ‚îÇ                    ‚îÇ  Public IP: Y   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Protocol: IKEv2
Encryption: AES256-CBC
Integrity: SHA256
DH Group: 14 (2048-bit)
PFS: Enabled
SA Lifetime: 27000 seconds (7.5 hours)
DPD Timeout: 45 seconds
```

### Route Tables

#### Azure Route Table (GatewaySubnet)

| Destination | Next Hop | Purpose |
|------------|----------|---------|
| 192.168.1.0/24 | VPN Gateway | On-premises network |
| 10.0.0.0/16 | Local | Azure VNet internal |
| 0.0.0.0/0 | Internet | Internet egress |

#### On-Premises Route Table

| Destination | Next Hop | Purpose |
|------------|----------|---------|
| 10.0.0.0/16 | VPN Tunnel | Azure VNet |
| 192.168.1.0/24 | Local | LAN |
| 0.0.0.0/0 | ISP Gateway | Internet |

### DNS Resolution
```
On-Premises:
  Kubernetes internal DNS: cluster.local
  Example: ibkr-gateway-0.ibkr-gateway-service.trading-bots.svc.cluster.local

Azure:
  App Service: spy-backend.azurewebsites.net
  SignalR: spy-signalr.service.signalr.net
  ACR: spyacr.azurecr.io
```

---

## üîÑ Data Flow

### 1. Market Data Ingestion (Real-Time)
```
IBKR Market Data Feed
    ‚Üì
IB Gateway Container (on-prem)
    ‚Üì TWS API (Port 4002)
Trading Bot Pods (on-prem)
    ‚Üì Parse & buffer
In-Memory Queue (Python asyncio)
```

**Latency**: 50-100ms (local network)

### 2. Anomaly Detection Pipeline
```
Trading Bot (detection algorithm)
    ‚Üì pandas/numpy processing
Anomaly Score Calculation
    ‚Üì If score > threshold
Signal Generation
    ‚Üì JSON payload
```

**Processing Time**: 100-200ms per option chain

### 3. Signal Broadcasting (Hybrid)
```
Trading Bot (on-prem)
    ‚Üì VPN S2S (IPsec)
Azure Backend API
    ‚Üì REST POST /signals
Azure SignalR Service
    ‚Üì WebSocket broadcast
Connected Clients (Dashboard)
```

**End-to-End Latency**: 250-400ms (95th percentile)

### 4. Trade Execution (Local)
```
Trading Bot receives signal
    ‚Üì Validation & risk checks
IB Gateway Container
    ‚Üì TWS API order placement
IBKR Paper Trading Account
    ‚Üì Order confirmation
Trading Bot (log execution)
```

**Execution Latency**: 50-150ms (local)

### 5. Telemetry Collection
```
All Pods (on-prem)
    ‚Üì Prometheus metrics (:9090/metrics)
Prometheus Server (on-prem)
    ‚Üì Scrape every 15s
Local storage (15 days retention)

AND

Fluentd DaemonSet
    ‚Üì Tail container logs
Buffer (local disk)
    ‚Üì VPN S2S
Azure Log Analytics
    ‚Üì Aggregation & indexing
Application Insights
```

---

## ‚öñÔ∏è Technical Decisions

### Decision 1: Why Hybrid Architecture?

**Options Considered:**
- ‚ùå Cloud-only (Azure VMs for trading bots)
- ‚ùå On-premises only (no cloud services)
- ‚úÖ Hybrid (edge + cloud)

**Decision:** Hybrid

**Rationale:**
- **Latency**: Trading execution requires <500ms end-to-end. Cloud round-trip adds 150-300ms.
- **Cost**: Azure VM Compute (B2s) = $35/month vs on-prem power = $5/month
- **Scalability**: Cloud provides managed services (SignalR, App Insights) without operational overhead
- **Best of Both**: Edge for performance, cloud for observability and management

**Trade-offs:**
- ‚ûï Optimal latency + cost
- ‚ûñ VPN dependency (single point of failure)
- ‚ûñ More complex networking

### Decision 2: Why Kubernetes for Single-Node?

**Options Considered:**
- ‚ùå Docker Compose
- ‚ùå Systemd services
- ‚úÖ Kubernetes (k3s/minikube)

**Decision:** Kubernetes

**Rationale:**
- **High Availability**: 3 replicas with auto-healing
- **Declarative**: GitOps-ready configuration
- **Industry Standard**: Skills transferable to AKS, EKS, GKE
- **Rich Ecosystem**: Helm, Prometheus Operator, Ingress controllers

**Trade-offs:**
- ‚ûï Production-grade orchestration
- ‚ûï Zero-downtime deployments
- ‚ûñ Higher learning curve
- ‚ûñ Resource overhead (~500MB for K8s components)

### Decision 3: Why Terraform over ARM/Bicep?

**Options Considered:**
- ‚ùå Azure ARM Templates
- ‚ùå Azure Bicep
- ‚ùå Pulumi
- ‚úÖ Terraform

**Decision:** Terraform

**Rationale:**
- **Cloud-Agnostic**: Can migrate to AWS/GCP without rewriting
- **Mature Ecosystem**: 3000+ providers, extensive documentation
- **State Management**: Remote state with locking prevents conflicts
- **Community**: Large community, abundant examples

**Trade-offs:**
- ‚ûï Portability across clouds
- ‚ûï Industry standard (most job postings)
- ‚ûñ Azure-specific features lag behind Bicep
- ‚ûñ State file management required

### Decision 4: Why Basic Tier Services?

**Options Considered:**
- VPN Gateway: Basic ($27/mo) vs Standard ($150/mo)
- ACR: Basic ($5/mo) vs Standard ($20/mo)
- App Service: B1 ($13/mo) vs S1 ($70/mo)

**Decision:** Basic tiers

**Rationale:**
- **Sufficient Performance**: Meets all latency and throughput requirements
- **Cost Optimization**: 70% savings vs Standard tiers
- **Scalable**: Can upgrade tiers without downtime if needed

**Limitations:**
- VPN: 100 Mbps (sufficient for telemetry)
- ACR: 10 GB storage (adequate for 5 images)
- App Service: Single instance (acceptable for demo)

---

## üìà Scalability & Resilience

### Horizontal Scaling

#### Trading Bot Pods
```yaml
# Current: 3 replicas
# Can scale to: 10 replicas (single-node limit)

kubectl scale deployment trading-bot --replicas=5 -n trading-bots
```

**Trigger**: CPU >70% for 5 minutes (HPA)

#### Cloud Services
```yaml
App Service:
  Current: B1 (1 instance)
  Scale to: S1 (3 instances) with auto-scale rules
  
SignalR:
  Current: Free tier (20 connections)
  Scale to: Standard (1000 connections per unit)
```

### Vertical Scaling
```yaml
# Resource limits per pod
Current:
  CPU: 500m (0.5 core)
  Memory: 512Mi

Vertical scaling:
  CPU: 1000m (1 core)
  Memory: 1Gi
```

### Auto-Healing
```yaml
Liveness Probe:
  Failure ‚Üí Pod restart (automatic)
  
Readiness Probe:
  Failure ‚Üí Remove from Service endpoints
  
Deployment Strategy:
  maxUnavailable: 0 ‚Üí Always N healthy replicas
```

### Disaster Recovery
```
Terraform State:
  Primary: Azure Blob Storage (LRS)
  Backup: Daily snapshots (7-day retention)
  
Kubernetes Manifests:
  Source: Git repository
  Restore: helm install from git tag
  
Data:
  Trading logs: Daily backup to Azure Blob
  Persistent volumes: Local (acceptable loss)
```

---

## üîí Security Architecture

### Identity & Access Management
```
Azure AD:
  Service Principal: Terraform automation (Contributor role)
  Managed Identity: App Service ‚Üí Key Vault (Secret Reader)
  
Kubernetes RBAC:
  ServiceAccount: trading-bot (limited permissions)
  NetworkPolicy: Deny all, allow specific
```

### Secrets Management
```
Storage:
  Azure Key Vault (primary)
  ‚îú‚îÄ‚îÄ IBKR credentials
  ‚îú‚îÄ‚îÄ VPN pre-shared key
  ‚îú‚îÄ‚îÄ SignalR connection string
  ‚îî‚îÄ‚îÄ ACR admin password
  
  Kubernetes Secrets (cached)
  ‚îî‚îÄ‚îÄ Base64-encoded, etcd-encrypted

Rotation:
  Manual: VPN PSK (quarterly)
  Automatic: Service Principal (90 days via Azure AD)
```

### Network Security
```
Azure NSG:
  Inbound: Whitelist on-premises IP only
  Outbound: Allow Azure services, deny all else
  
Kubernetes NetworkPolicy:
  trading-bots namespace:
    ‚îú‚îÄ‚îÄ Allow: Pod-to-Pod within namespace
    ‚îú‚îÄ‚îÄ Allow: To IBKR Gateway (port 4002)
    ‚îî‚îÄ‚îÄ Deny: All other traffic
  
  monitoring namespace:
    ‚îú‚îÄ‚îÄ Allow: Prometheus scraping (all namespaces)
    ‚îî‚îÄ‚îÄ Deny: Ingress from external
```

### Container Security
```
Image Scanning:
  Tool: Trivy (GitHub Actions)
  Frequency: Every build
  Policy: Block HIGH/CRITICAL vulnerabilities
  
Runtime:
  User: Non-root (UID 1000)
  Filesystem: Read-only root
  Capabilities: Dropped all, add NET_BIND_SERVICE only
```

---

## üìä Monitoring & Observability

### Metrics Stack
```
Prometheus:
  Scrape Interval: 15s
  Retention: 15 days
  Targets:
    - trading-bot pods (:8080/metrics)
    - ibkr-gateway (:9090/metrics)
    - node-exporter (host metrics)
    - kube-state-metrics (K8s objects)
  
Grafana:
  Dashboards:
    - Kubernetes Cluster Overview
    - Trading Bot Performance
    - IBKR Gateway Health
    - Application Metrics
```

### Custom Metrics
```python
# Trading Bot Metrics
spy_bot_trades_total (counter)
spy_bot_signals_received_total (counter)
spy_bot_anomalies_detected_total (counter)
spy_bot_signal_latency_seconds (histogram)
spy_bot_execution_latency_seconds (histogram)
spy_bot_errors_total (counter)
spy_bot_active_positions (gauge)

# IBKR Gateway Metrics
ibkr_connection_status (gauge)  # 1=connected, 0=disconnected
ibkr_api_response_time_seconds (histogram)
ibkr_requests_total (counter)
```

### Logging Architecture
```
Container Logs:
  Format: JSON structured
  Fields: timestamp, level, message, pod_name, namespace
  
Flow:
  stdout/stderr ‚Üí Docker logs
    ‚Üì
  Fluentd DaemonSet (tail logs)
    ‚Üì
  Local buffer (10GB, failover)
    ‚Üì VPN S2S
  Azure Log Analytics
    ‚Üì
  Application Insights (correlation)
```

### Distributed Tracing
```
Instrumentation:
  Backend API: OpenTelemetry Python SDK
  Trace Context: W3C standard (traceparent header)
  
Spans:
  1. HTTP request received (App Service)
  2. IBKR data fetch (trading bot)
  3. Anomaly detection algorithm
  4. Signal publish to SignalR
  5. WebSocket broadcast to clients
  
Backend: Application Insights
Query: End-to-end latency per operation
```

### Alerting Rules
```yaml
Prometheus Alerts:
  TradingBotDown:
    expr: up{job="trading-bot"} == 0
    for: 2m
    severity: critical
  
  HighSignalLatency:
    expr: histogram_quantile(0.95, spy_bot_signal_latency_seconds) > 0.5
    for: 5m
    severity: warning
  
  IBKRDisconnected:
    expr: ibkr_connection_status == 0
    for: 1m
    severity: critical
  
  HighMemoryUsage:
    expr: container_memory_usage_bytes / container_spec_memory_limit_bytes > 0.9
    for: 5m
    severity: warning
```

---

## üéØ Performance Benchmarks

| Operation | Target | Measured | Status |
|-----------|--------|----------|--------|
| **Market data latency** | <100ms | 75ms (avg) | ‚úÖ |
| **Anomaly detection** | <200ms | 150ms (avg) | ‚úÖ |
| **Signal broadcast** | <500ms | 350ms (p95) | ‚úÖ |
| **Trade execution** | <150ms | 95ms (avg) | ‚úÖ |
| **VPN round-trip** | <30ms | 22ms (avg) | ‚úÖ |
| **Pod startup** | <30s | 18s (avg) | ‚úÖ |
| **Rolling update** | 0 downtime | 0s (verified) | ‚úÖ |

---

## üîÆ Future Enhancements

### Short-Term (Next 3 Months)
- [ ] Add Horizontal Pod Autoscaler (HPA)
- [ ] Implement blue-green deployment strategy
- [ ] Add Istio service mesh for advanced traffic control
- [ ] Configure backup VPN tunnel (active-passive)

### Medium-Term (6 Months)
- [ ] Multi-region deployment (Azure + AWS)
- [ ] Real-time strategy backtesting pipeline
- [ ] Machine learning anomaly model (TensorFlow)
- [ ] Mobile app with push notifications

### Long-Term (12 Months)
- [ ] Multi-cloud orchestration (Azure + AWS + GCP)
- [ ] Kubernetes cluster expansion (3-node HA)
- [ ] Real money trading (production IBKR)
- [ ] Open-source framework for others

---

**Last Updated:** December 2025
**Architecture Version:** 1.0  
**Status:** Production-Ready